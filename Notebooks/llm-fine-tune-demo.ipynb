{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Login to weights & biases for experiment tracking","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-12T07:50:16.878953Z","iopub.execute_input":"2024-11-12T07:50:16.879265Z","iopub.status.idle":"2024-11-12T07:50:17.190897Z","shell.execute_reply.started":"2024-11-12T07:50:16.879231Z","shell.execute_reply":"2024-11-12T07:50:17.190049Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e95349e693c44764af6d86511e29c078"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install datasets\n!pip install peft\n!pip install bitsandbytes\n!pip install trl\n!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2024-11-12T07:50:45.936562Z","iopub.execute_input":"2024-11-12T07:50:45.936987Z","iopub.status.idle":"2024-11-12T07:52:01.644286Z","shell.execute_reply.started":"2024-11-12T07:50:45.936949Z","shell.execute_reply":"2024-11-12T07:52:01.643306Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\nCollecting trl\n  Downloading trl-0.12.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from trl) (0.34.2)\nRequirement already satisfied: datasets>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from trl) (3.0.1)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\nCollecting transformers>=4.46.0 (from trl)\n  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.25.1)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.21.0->trl) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.9.5)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (0.20.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.34.0->trl) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\nDownloading trl-0.12.0-py3-none-any.whl (310 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers, trl\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed transformers-4.46.2 trl-0.12.0\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.3)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.15.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport os\n\nimport transformers\nimport torch\n\nimport accelerate\nfrom transformers import BitsAndBytesConfig\nimport bitsandbytes\nfrom accelerate import Accelerator\n\nimport pandas as pd\nfrom pathlib import Path\nfrom datasets import load_dataset\nfrom peft import PeftModel, get_peft_model,LoraConfig,TaskType,prepare_model_for_kbit_training\nfrom torch import cuda, bfloat16\nfrom contextlib import nullcontext\nfrom tqdm import tqdm\nfrom itertools import chain\nfrom torch.utils.data import Dataset\nimport datasets\n\nfrom trl import SFTTrainer, setup_chat_format, SFTConfig\n\nfrom transformers import TrainingArguments\nfrom transformers import Trainer\nfrom transformers import LlamaForCausalLM, LlamaConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,AutoModelForSeq2SeqLM\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-11-12T07:52:01.646630Z","iopub.execute_input":"2024-11-12T07:52:01.647444Z","iopub.status.idle":"2024-11-12T07:52:21.999127Z","shell.execute_reply.started":"2024-11-12T07:52:01.647395Z","shell.execute_reply":"2024-11-12T07:52:21.998280Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"wandb.login()\nepoch = 2\nlr = 0.002\nbs = 1\nquantized = 'bfloat16'\nmax_length = 512\n#  2. Capture a dictionary of hyperparameters\n\nrun = wandb.init(\n    # Set the project where this run will be logged\n    project=\"testing\",\n    config = {\"quantized\": quantized},\n    job_type=\"train\" \n)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T03:44:13.954843Z","iopub.execute_input":"2024-11-08T03:44:13.955247Z","iopub.status.idle":"2024-11-08T03:44:20.250287Z","shell.execute_reply.started":"2024-11-08T03:44:13.955204Z","shell.execute_reply":"2024-11-08T03:44:20.249016Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      3\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.002\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:85\u001b[0m, in \u001b[0;36mlogin\u001b[0;34m(anonymous, key, relogin, host, force, timeout, verify)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39msetup()\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39m_noop:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m configured \u001b[38;5;241m=\u001b[39m \u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43manonymous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manonymous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelogin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelogin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_setup\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:347\u001b[0m, in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logged_in\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key:\n\u001b[0;32m--> 347\u001b[0m     \u001b[43mwlogin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wlogin\u001b[38;5;241m.\u001b[39m_key \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:274\u001b[0m, in \u001b[0;36m_WandbLogin.prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt_api_key\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m     key, status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m ApiKeyStatus\u001b[38;5;241m.\u001b[39mNOTTY:\n\u001b[1;32m    276\u001b[0m         directive \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb login [your_api_key]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39m_cli_only_mode\n\u001b[1;32m    279\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb.login(key=[your_api_key])\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_login.py:253\u001b[0m, in \u001b[0;36m_WandbLogin._prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[43mapikey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_api_key\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[43mno_offline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m            \u001b[49m\u001b[43mno_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# invalid key provided, try again\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermerror(e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/apikey.py:164\u001b[0m, in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    158\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermlog(\n\u001b[1;32m    159\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogging into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. (Learn how to deploy a W&B server locally: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwburls\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb_server\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         )\n\u001b[1;32m    161\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mtermlog(\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can find your API key in your browser here: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapp_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/authorize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m     )\n\u001b[0;32m--> 164\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43minput_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_ask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    165\u001b[0m write_key(settings, key, api\u001b[38;5;241m=\u001b[39mapi)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m key  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/click/termui.py:164\u001b[0m, in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/click/termui.py:147\u001b[0m, in \u001b[0;36mprompt.<locals>.prompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hide_input:\n\u001b[1;32m    146\u001b[0m     echo(\u001b[38;5;28;01mNone\u001b[39;00m, err\u001b[38;5;241m=\u001b[39merr)\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Abort() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mAbort\u001b[0m: "],"ename":"Abort","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# load model","metadata":{}},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,AutoModelForSeq2SeqLM\nimport torch\nimport bitsandbytes\n\n\nconfig = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\n\n# Define the model name or checkpoint path\nmodel_name = \"meta-llama/Llama-3.2-1B\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(model_name,device_map='cuda', quantization_config=config)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-11-12T07:52:22.000171Z","iopub.execute_input":"2024-11-12T07:52:22.000448Z","iopub.status.idle":"2024-11-12T07:54:26.137732Z","shell.execute_reply.started":"2024-11-12T07:52:22.000419Z","shell.execute_reply":"2024-11-12T07:54:26.136923Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0dacd88425245a290fe5d11a1f7dbc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b474adfa9d1c462195480cb816ac3788"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45a80a2f644a4f45a661e670e4d8502b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/885 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1191c9a0f494506a77578a9e24ce2ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f3f20f2559d4c46aa66e9719d9216bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"868bd0d895af433fa25e87e6ae130408"}},"metadata":{}}]},{"cell_type":"markdown","source":"# dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"knkarthick/dialogsum\")\nprint(dataset)\nsmall_dataset = dataset['train'].shuffle(seed=42).select(range(100))\ndataset[\"train\"] = small_dataset\n\nsmall_dataset = dataset['validation'].shuffle(seed=42).select(range(20))\ndataset[\"validation\"] = small_dataset\n\nsmall_dataset = dataset.get('test').shuffle(seed=42).select(range(20))\ndataset[\"test\"] = small_dataset\nprint('--------------')\nprint(f'processed dataset: {dataset}')","metadata":{"execution":{"iopub.status.busy":"2024-11-12T07:58:18.954797Z","iopub.execute_input":"2024-11-12T07:58:18.955216Z","iopub.status.idle":"2024-11-12T07:58:22.317908Z","shell.execute_reply.started":"2024-11-12T07:58:18.955183Z","shell.execute_reply":"2024-11-12T07:58:22.316743Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1c397f07bc546b5a3bae6c4278b58d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70079ad8e9784a36b194e497419e06e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49f15b348adf4f76b3527768e072b8b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827547d797954c819118fa0530048fe8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06d0aace97b64e3ca257fd3cc9983e92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffea1472e70146968107c4a639532154"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9df50943ced4a3b9d6771e3a85e0e1c"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 12460\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1500\n    })\n})\n--------------\nprocessed dataset: DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 100\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 20\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 20\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token\ndef format_template(row):\n    dialogue = row[\"dialogue\"].strip('\\n')\n    prompt = f'''\n    Summarize the following conversation.\n    \n    Conversation:\n    {dialogue}\n\n    Summary: \n    {row[\"summary\"]}\n    '''\n\n    row[\"text\"] = prompt + EOS_TOKEN\n    return row\n\ndataset = dataset.map(\n    format_template,\n    num_proc=4,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T07:58:30.816348Z","iopub.execute_input":"2024-11-12T07:58:30.817230Z","iopub.status.idle":"2024-11-12T07:58:31.977007Z","shell.execute_reply.started":"2024-11-12T07:58:30.817188Z","shell.execute_reply":"2024-11-12T07:58:31.975983Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da2f181d812245ceb6a456313c814562"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"542de8f1e2384b389b70392503bf8028"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea9f4f8975464f9a87c200b3fb7e7117"}},"metadata":{}}]},{"cell_type":"code","source":"dataset['train']['text'][1]","metadata":{"execution":{"iopub.status.busy":"2024-11-12T07:58:33.451406Z","iopub.execute_input":"2024-11-12T07:58:33.452191Z","iopub.status.idle":"2024-11-12T07:58:33.460311Z","shell.execute_reply.started":"2024-11-12T07:58:33.452150Z","shell.execute_reply":"2024-11-12T07:58:33.459393Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"\"\\n    Summarize the following conversation.\\n    \\n    Conversation:\\n    #Person1#: Mister Ewing said we should show up at the conference center at 4:00 o'clock, right?\\n#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are coming, and he wants to make a good impression on them. How are you getting there?\\n#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is construction on the highway. What about you?\\n#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center only once, and I'm not sure if I can find my way around there.\\n\\n    Summary: \\n    #Person1# and #Person2# plan to take the underground together to the conference center because Mr. Ewing asks them not to be late.\\n    <|end_of_text|>\""},"metadata":{}}]},{"cell_type":"markdown","source":"# Initial Response","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nSummarize the following conversation.\n\nCoversation:\n#Person1#: Mister Ewing said we should show up at the conference center at 4:00 o'clock, right?\n#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are coming, and he wants to make a good impression on them. How are you getting there?\n#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is construction on the highway. What about you?\n#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center only once, and I'm not sure if I can find my way around there.\n\nSummary:\n\"\"\"\ninputs = tokenizer(prompt, return_tensors='pt').to('cuda')\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"], \n        max_new_tokens=200,\n    )[0], \n    skip_special_tokens=True\n)\n#print(f'INPUT PROMPT:\\n{prompt}')\nprint(output)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-07T02:38:16.365652Z","iopub.execute_input":"2024-11-07T02:38:16.366389Z","iopub.status.idle":"2024-11-07T02:38:26.945974Z","shell.execute_reply.started":"2024-11-07T02:38:16.366346Z","shell.execute_reply":"2024-11-07T02:38:26.945046Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"\nSummarize the following conversation.\n\nCoversation:\n#Person1#: Mister Ewing said we should show up at the conference center at 4:00 o'clock, right?\n#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are coming, and he wants to make a good impression on them. How are you getting there?\n#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is construction on the highway. What about you?\n#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center only once, and I'm not sure if I can find my way around there.\n\nSummary:\n#Person1# and #Person2# are both attending a conference. #Person1# wants to show up at the conference center at 4:00 o'clock, #Person2# wants to show up at 4:00 o'clock, but #Person1# is worried that he will be late. #Person2# is worried that he will be late. #Person1# asks #Person2# if he can show up at the conference center at 4:00 o'clock. #Person2# says that he will show up at 4:00 o'clock, but he also asks #Person1# if he can show up at 4:00 o'clock. #Person1# says that he will show up at 4:00 o'clock. #Person2# then asks #Person1# if he can show up at 4:00 o'clock. #Person1# says that he will show up at 4:00 o'clock.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n)\n\n\ntraining_arguments = SFTConfig(\n    output_dir='./trained',\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"adamw_torch\",\n    num_train_epochs=20,\n    eval_strategy=\"steps\",\n    eval_steps=10,\n    logging_steps=5,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    dataset_text_field=\"text\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    max_seq_length=512,\n    report_to=\"wandb\",\n    save_strategy=\"no\"\n)\n\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T02:38:57.911751Z","iopub.execute_input":"2024-11-07T02:38:57.912665Z","iopub.status.idle":"2024-11-07T02:38:58.789730Z","shell.execute_reply.started":"2024-11-07T02:38:57.912621Z","shell.execute_reply":"2024-11-07T02:38:58.788980Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7330dd50363440d5b21b64e24b9cdb21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7f0e390877243e5960b4459c55345dd"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-07T02:39:00.292966Z","iopub.execute_input":"2024-11-07T02:39:00.293362Z","iopub.status.idle":"2024-11-07T03:00:12.979676Z","shell.execute_reply.started":"2024-11-07T02:39:00.293325Z","shell.execute_reply":"2024-11-07T03:00:12.978705Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 21:10, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.810300</td>\n      <td>1.842821</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.701400</td>\n      <td>1.713877</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.797000</td>\n      <td>1.702748</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.674800</td>\n      <td>1.686877</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.589000</td>\n      <td>1.671448</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.348000</td>\n      <td>1.695241</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.346600</td>\n      <td>1.696154</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.525200</td>\n      <td>1.719518</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.491200</td>\n      <td>1.739847</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.305700</td>\n      <td>1.720833</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.874100</td>\n      <td>2.026942</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.124500</td>\n      <td>1.858907</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.174200</td>\n      <td>1.954076</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.990200</td>\n      <td>1.985843</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.759400</td>\n      <td>1.917713</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.432900</td>\n      <td>2.305204</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.531700</td>\n      <td>2.219715</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.709300</td>\n      <td>2.188003</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.758900</td>\n      <td>2.169667</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.465700</td>\n      <td>2.242879</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.216900</td>\n      <td>2.539578</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.422900</td>\n      <td>2.477221</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.297100</td>\n      <td>2.554958</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.444300</td>\n      <td>2.582812</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.288700</td>\n      <td>2.434701</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.127700</td>\n      <td>3.068316</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.167100</td>\n      <td>2.567595</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.226800</td>\n      <td>2.855495</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.257100</td>\n      <td>2.760036</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.169800</td>\n      <td>2.670570</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.097600</td>\n      <td>2.894782</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.130000</td>\n      <td>2.972682</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.138500</td>\n      <td>2.854151</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.164700</td>\n      <td>3.011258</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.117700</td>\n      <td>2.920269</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.066600</td>\n      <td>3.011379</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.101200</td>\n      <td>3.240244</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.090500</td>\n      <td>3.040962</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.095200</td>\n      <td>3.085981</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.085900</td>\n      <td>2.968612</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.070100</td>\n      <td>3.163550</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.061500</td>\n      <td>3.255720</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.084300</td>\n      <td>3.244455</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.081900</td>\n      <td>3.110414</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.063500</td>\n      <td>3.074810</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.041900</td>\n      <td>3.360531</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.050300</td>\n      <td>3.427071</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.047500</td>\n      <td>3.352235</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.052700</td>\n      <td>3.359006</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.053100</td>\n      <td>3.221243</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.048000</td>\n      <td>3.364426</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.042800</td>\n      <td>3.491034</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.048500</td>\n      <td>3.527903</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.055000</td>\n      <td>3.434459</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.050000</td>\n      <td>3.338235</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.031600</td>\n      <td>3.495344</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.043500</td>\n      <td>3.561597</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.037900</td>\n      <td>3.437909</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.047700</td>\n      <td>3.392509</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.044100</td>\n      <td>3.495381</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.037400</td>\n      <td>3.532995</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.033500</td>\n      <td>3.612488</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.029800</td>\n      <td>3.587810</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.045000</td>\n      <td>3.567725</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.037900</td>\n      <td>3.540367</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.033300</td>\n      <td>3.502823</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.029400</td>\n      <td>3.698005</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.027900</td>\n      <td>3.747801</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.030500</td>\n      <td>3.612351</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.037100</td>\n      <td>3.653805</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.024900</td>\n      <td>3.779137</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.025900</td>\n      <td>3.809807</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.024700</td>\n      <td>3.783745</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.028100</td>\n      <td>3.797710</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.030700</td>\n      <td>3.840591</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.024100</td>\n      <td>3.894629</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.025900</td>\n      <td>3.957543</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.021700</td>\n      <td>3.939765</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.024700</td>\n      <td>3.913184</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.032100</td>\n      <td>3.908826</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.022000</td>\n      <td>3.919059</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.021300</td>\n      <td>3.952305</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.019200</td>\n      <td>4.006373</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.023200</td>\n      <td>4.032105</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.035800</td>\n      <td>4.042223</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.021400</td>\n      <td>4.060576</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.021600</td>\n      <td>4.077204</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.018500</td>\n      <td>4.085576</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.022300</td>\n      <td>4.102291</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.030700</td>\n      <td>4.108879</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.024500</td>\n      <td>4.111733</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.019200</td>\n      <td>4.133990</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.019400</td>\n      <td>4.134601</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.020900</td>\n      <td>4.139118</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.023300</td>\n      <td>4.142247</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.022800</td>\n      <td>4.139961</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.019600</td>\n      <td>4.143752</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.018700</td>\n      <td>4.142825</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.019400</td>\n      <td>4.145325</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.024900</td>\n      <td>4.142639</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.29391394935548304, metrics={'train_runtime': 1272.1633, 'train_samples_per_second': 1.572, 'train_steps_per_second': 0.786, 'total_flos': 3142478157004800.0, 'train_loss': 0.29391394935548304, 'epoch': 20.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Trained Response","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"\nSummarize the following conversation.\n\nConversation:\n#Person1#: Mister Ewing said we should show up at the conference center at 4:00 o'clock, right?\n#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are coming, and he wants to make a good impression on them. How are you getting there?\n#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is construction on the highway. What about you?\n#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center only once, and I'm not sure if I can find my way around there.\n\nSummary:\n\"\"\"\ninputs = tokenizer(prompt, return_tensors='pt').to('cuda')\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"], \n        max_new_tokens=200,\n    )[0], \n    skip_special_tokens=True\n)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(output)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-07T03:00:12.981325Z","iopub.execute_input":"2024-11-07T03:00:12.981637Z","iopub.status.idle":"2024-11-07T03:00:15.502391Z","shell.execute_reply.started":"2024-11-07T03:00:12.981598Z","shell.execute_reply":"2024-11-07T03:00:15.501315Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"INPUT PROMPT:\n\nSummarize the following conversation.\n\nConversation:\n#Person1#: Mister Ewing said we should show up at the conference center at 4:00 o'clock, right?\n#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are coming, and he wants to make a good impression on them. How are you getting there?\n#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is construction on the highway. What about you?\n#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center only once, and I'm not sure if I can find my way around there.\n\nSummary:\n\n\nSummarize the following conversation.\n\nConversation:\n#Person1#: Mister Ewing said we should show up at the conference center at 4:00 o'clock, right?\n#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are coming, and he wants to make a good impression on them. How are you getting there?\n#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is construction on the highway. What about you?\n#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center only once, and I'm not sure if I can find my way around there.\n\nSummary:\n#Person1# and #Person2# plan to take the underground together to the conference center because Mr. Ewing asks them not to be late.\n \n    \n","output_type":"stream"}]},{"cell_type":"markdown","source":"alternatively, you could push to huggingface using: \n\n*Take note that the files you are pushing is only the adapter. You will need to merge it with the base model later on.","metadata":{}},{"cell_type":"code","source":"trainer.tokenizer.push_to_hub('ww-fine-tuned-adapter-v9',private=True)\ntrainer.model.push_to_hub('ww-fine-tuned-adapter-v9',private=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T03:00:44.396212Z","iopub.execute_input":"2024-11-07T03:00:44.397089Z","iopub.status.idle":"2024-11-07T03:00:50.901417Z","shell.execute_reply.started":"2024-11-07T03:00:44.397035Z","shell.execute_reply":"2024-11-07T03:00:50.900480Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c05e72f0e8de4ef88797a0a6da9d7012"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abead7990a3d4ad6a648ddba79a54d14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/22.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c4c8eca8d5c4e81a2d3511c546ad65a"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/WeiWen21/ww-fine-tuned-adapter-v9/commit/356609010c120dc9757de139a8c4dcdc27761a4a', commit_message='Upload model', commit_description='', oid='356609010c120dc9757de139a8c4dcdc27761a4a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/WeiWen21/ww-fine-tuned-adapter-v9', endpoint='https://huggingface.co', repo_type='model', repo_id='WeiWen21/ww-fine-tuned-adapter-v9'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"run = wandb.init(\n    # Set the project where this run will be logged\n    project=\"llama-fine-tune\",\n    job_type=\"evaluate\" \n)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T07:59:04.912632Z","iopub.execute_input":"2024-11-12T07:59:04.913484Z","iopub.status.idle":"2024-11-12T07:59:35.082042Z","shell.execute_reply.started":"2024-11-12T07:59:04.913447Z","shell.execute_reply":"2024-11-12T07:59:35.080991Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113651622220761, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec9c8cc8f3cd44ec92c36ce8e4926753"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241112_075931-o21g22t4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ds-team21/llama-fine-tune/runs/o21g22t4' target=\"_blank\">twilight-leaf-23</a></strong> to <a href='https://wandb.ai/ds-team21/llama-fine-tune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ds-team21/llama-fine-tune' target=\"_blank\">https://wandb.ai/ds-team21/llama-fine-tune</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ds-team21/llama-fine-tune/runs/o21g22t4' target=\"_blank\">https://wandb.ai/ds-team21/llama-fine-tune/runs/o21g22t4</a>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install evaluate\n!pip install bert_score","metadata":{"execution":{"iopub.status.busy":"2024-11-12T07:59:37.017938Z","iopub.execute_input":"2024-11-12T07:59:37.018338Z","iopub.status.idle":"2024-11-12T08:00:03.589107Z","shell.execute_reply.started":"2024-11-12T07:59:37.018301Z","shell.execute_reply":"2024-11-12T08:00:03.588073Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nCollecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.4.0)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.2.2)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.46.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.66.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2024.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.25.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.20.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (10.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2024.8.30)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.64k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e45d6e912eb4fcfb7a78f09624de458"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03773d8c08d247c2806ea2566606b0e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb429e89ff324388a087fc519ad2c6ff"}},"metadata":{}}]},{"cell_type":"code","source":"import evaluate\n#evaluate.list_evaluation_modules()\nbleu = evaluate.load('evaluate-metric/google_bleu')\nbert = evaluate.load('evaluate-metric/bertscore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\ngt = []\nwandb_summary = []\n\nfor row in dataset['test']:\n    \n    prompt = f\"\"\"\n    Summarize the following conversation.\n\n    Conversation:\n    {row['dialogue']}\n    \n    Summary:\n    \"\"\"\n    inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n    output = tokenizer.decode(\n        model.generate(\n            inputs[\"input_ids\"], \n            max_new_tokens=200,\n        )[0], \n        skip_special_tokens=True\n    )\n\n\n    print(output.split(\"Summary:\")[1].strip('/n'))\n    current_pred = output.split(\"Summary:\")[1]\n    current_gt = row['summary']\n    predictions.append(current_pred)\n    gt.append(current_gt)\n    bleu_score = bleu.compute(predictions=[current_pred],references=[current_gt])['google_bleu']\n    bert_score = bert.compute(predictions=[current_pred],references=[current_gt],lang='en')\n    precision, recall, f1 = bert_score['precision'][0], bert_score['recall'][0], bert_score['f1'][0]\n\n    wandb.log(\n        {'bleu_score':bleu_score,\n        'precision':precision,\n        'recall':recall,\n        'f1':f1}\n    )\n    \n    wandb_summary.append([prompt,current_gt,current_pred,bleu_score,precision,recall,f1])\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-12T08:00:39.853789Z","iopub.execute_input":"2024-11-12T08:00:39.854162Z","iopub.status.idle":"2024-11-12T08:02:56.777141Z","shell.execute_reply.started":"2024-11-12T08:00:39.854129Z","shell.execute_reply":"2024-11-12T08:02:56.776136Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     #Person1# and #Person2# are talking about haj, which is a pilgrimage to Mecca for muslims. #Person1# asks #Person2# what muslims do on the pilgrimage. #Person2# says that muslims must face towards Mecca and that the sacred black stone is around the large, sacred well. #Person1# asks #Person2# if there is something magical about the place. #Person2# says that people believe that going to Lourdes will cure them and that there is no magic in the place.\n    \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a01df79938694389824e0ccb3f1ed6fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf249563091144578c36db5f9b29d7e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b370200bd8345da96e49a04f1d4d55a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ec1cc4a3a514ebca689a201d5240413"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a34f3a7f84ad49f0abe00948d83d8899"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e635ea07156044d480e9dcefadfdb6c5"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     #Person1# wants to send a package by first-class mail. #Person2# asks #Person1# if he wants it insured. #Person1# says yes, for $50. #Person2# asks for the stamps and money orders. #Person1# wants to get them at the stamp window.\n    \n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     Lily will take part in the picnic at the river.\n     Lily will bring nothing to the picnic.\n     Lily and Person1 will go to the river, go around and have supper.\n     Lily and Person1 will bring everything to the picnic.\n     \n     #Person1#: I think we can go to the river, go around and have supper.\n     #Person2#: What should I bring?\n     #Person1#: Nothing. Just wear comfortable clothes and good shoes for walking. We'll bring everything.\n     \n     \n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     #Person1# can get #Person2# something to drink. #Person2# doesn't drink much, so #Person1# suggests an aperitif. #Person1# then suggests a mixed drink and a non-alcoholic cocktail. #Person2# wants to try the Singer, which is made with lime juice and grenadine. #Person1# then serves the drink and enjoys #Person2#'s company.\n    \n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     Lin Fang and Lucy have different favorite subjects. Lin Fang likes English, while Lucy likes Chinese and science. Lucy likes Chinese the most.\n     Lin Fang likes science the least.\n     Lin Fang and Lucy both like PE.\n     Lin Fang likes Chinese the most.\n     Lucy likes Chinese the most.\n     Lin Fang likes science the least.\n     Lin Fang likes PE the most.\n     Lucy likes science the most.\n     Lin Fang likes Chinese the most.\n     Lin Fang likes English the least.\n     Lucy likes English the most.\n     Lin Fang likes science the least.\n     Lin Fang likes PE the most.\n     Lucy likes Chinese the most.\n     Lin Fang likes English the most.\n     Lin Fang likes science the least.\n     Lin Fang likes PE the most.\n     Lucy likes Chinese the most.\n     Lin Fang likes English the most.\n     Lin Fang likes science the least.\n     Lin Fang likes PE the most.\n     Lucy likes Chinese the most.\n     Lin Fang likes English the most.\n     Li\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     #Person1# is talking about a neighbor he met at the park. They are hitting it off well. #Person2# is thinking about inviting #Person2#'s neighbor #Person2# to dinner. #Person1# agrees with #Person2#.\n    \n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     Person 1: How can I help you?\n     Person 2: I seem to have lost my train ticket.\n     Person 1: What's your destination?\n     Person 2: I'm supposed to be going to shanghai.\n     Person 1: Ok. When is your train supposed to leave?\n     Person 2: It's supposed to leave in 30 minutes.\n     Person 1: OH, dear. What's your last name, please?\n     Person 2: S M I T H.\n     Person 1: And your passport number?\n     Person 2: Z3264356.\n     Person 1: Let me see... it doesn't look like we have any information about your ticket here.\n     Person 2: Is there anything you can do for me? If I don't make it to shanghai in time, my wife is going go kill me.\n     Person 1: Oh dear. I'm really sorry, sir\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     Person 1. You're going to set up your own law office, aren't you?\n     Person 2. Yes. After so many years of hard work, I'd rather have an office of my own.\n     Person 1. If you need help, don't hesitate to ask me.\n     Person 2. I'll be very glad if you would help.\n     Person 1. I'd like to wish you every success in your new venture.\n     Person 2. Thank you. I wish I would.\n     Person 1. Good luck to you.\n     Person 2. Good luck to you.\n     \n     \n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     #Person1# is calling #Person2# who he knows is calling him tenth time. #Person1# tells #Person2# to stop calling him and to call the police if #Person2# calls again. #Person1# threatens to arrest #Person2# if he calls again.\n    \n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     Person 1: Good afternoon, what can I do for you?\n     Person 2: Yes, please. I would like to know something about the driving courses.\n     Person 1: Well, We have short full time courses during the summer. Are you interested in them?\n     Person 2: No, I am free only at weekends.\n     Person 1: Then there are weekend courses. The course starts at 8:00 every Saturday and Sunday morning.\n     Person 2: Sounds fine. What about the coaches?\n     Person 1: We have very excellent coaches here and some of them have been teaching for 20 years.\n     Person 2: Good. How many hours of training should I have each day?\n     Person 1: 3 hours in the morning and 2 in the afternoon. It ends at 6:00 PM.\n     Person 2: Then how many people share a training car?\n     Person 1: Usually 3 and\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     Person1: Are you familiar with American-styled accounting?\n     Person2: I am afraid not. I haven't worked in an American company so far.\n     Person1: What are the most fundamental concepts underlying the accounting process?\n     Person2: The first is accounting entity, and the second is going concern. The third is measuring unit. The fourth is accounting period, and the fifth is objectivity.\n     \n     Person1: I see. How about the accounting cycle? Do you know about it?\n     Person2: Yes, I do. It consists of four steps: cash, assets, liabilities, and equity. The first two are the accounting entries, and the last two are the journal entries.\n     \n     Person1: Okay, I understand. So, what is the difference between an accounting cycle and an accounting period?\n     Person2: An accounting period is a period of time during which an entity records its transactions. An accounting cycle is a series of transactions that an entity\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     #Person1# thinks spring is finally here, but it's still very cold at night. #Person2# agrees, but #Person1# thinks it's too cold to just sit in the sun. #Person1# suggests they go outside and play some ping-pong.\n    \n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     Mark needs someone to study with and Maggie needs someone to keep her awake. So they decide to be study partners. They go to the library.\n     Mark needs someone to study with and Maggie needs someone to keep him awake. So they decide to be study partners. They go to the library.\n     Mark needs someone to study with and Maggie needs someone to keep him awake. So they decide to be study partners. They go to the library.\n     Mark needs someone to study with and Maggie needs someone to keep him awake. So they decide to be study partners. They go to the library.\n     Mark needs someone to study with and Maggie needs someone to keep him awake. So they decide to be study partners. They go to the library.\n     Mark needs someone to study with and Maggie needs someone to keep him awake. So they decide to be study partners. They go to the library.\n     Mark needs someone to study with and Maggie needs someone to keep him awake. So they decide to be\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     Tom and Emm are talking about Emm's new assistant. Tom finds her to be'stuck up', but Emm thinks Tom is being unfair. They argue about it.\n     Tom says that Emm is like that with everyone. Emm thinks Tom is being unfair. They argue some more.\n     Finally, they agree that Emm is a nice person, but she does need to improve her social skills.\n     Tom and Emm agree that Emm is a nice person, but she does need to improve her social skills.\n     They agree that Emm is a nice person, but she does need to improve her social skills.\n     Finally, they agree that Emm is a nice person, but she does need to improve her social skills.\n     They agree that Emm is a nice person, but she does need to improve her social skills.\n     Finally, they agree that Emm is a nice person, but she does need to improve her social skills.\n     They agree that Emm is a nice person, but she does need to\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     #Person1# likes football, basketball, and swimming. #Person2# likes football, basketball, and tennis. #Person1# likes golf, but it is too expensive for some people. #Person2# likes golf, but there are not many people who enjoy it. #Person1# knows people who play it regularly. #Person2# knows people who like golf, but there are not many of them.\n    \n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     #Person1# and #Person2# are talking about #Person2#'s chicken pox.\n     #Person1# tells #Person2# that #Person2# has chicken pox and that #Person2# is a biohazard. #Person2# says that #Person1# always blows things out of proportion.\n     #Person1# tells #Person2# to take an oatmeal bath.\n     \n     #Person1# is a doctor. #Person2# is not a doctor.\n     \n     #Person1# is older than #Person2#.\n     \n     #Person2# is a man.\n     \n     #Person1# and #Person2# are in the United States.\n     \n     #Person1# is talking about #Person2#'s chicken pox. #Person2# is talking about #Person1#'s diagnosis of chicken pox.\n     \n     #Person1# is talking about #Person2#'s rash. #Perso\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     Person 1 asks Person 2 to take off his jacket and shirt. Then Person 2 holds up Person 1's right arm. Person 1 asks Person 2 if it hurts. Person 2 says yes, and Person 1 asks if Person 2 doesn't feel anything else. Person 2 says no, and Person 1 asks if Person 2 can stay in the hospital for the night.\n     Person 1 thinks Person 2 doesn't feel anything else because Person 2 doesn't feel anything in Person 2's legs.\n     Person 2 stays in the hospital for the night.\n     Person 1 takes Person 2's shoulder X-ray in the morning.\n     Person 1 and Person 2 decide to have Person 2 stay in the hospital for the next few days.\n     Person 1 explains to the nurse that Person 2 doesn't feel anything else.\n     Person 1 and Person 2 take Person 2 to the hospital every day.\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     #Person1# likes football, basketball, swimming and golf. #Person2# likes football, basketball, swimming and tennis. #Person1# thinks that extreme sports are only for a small number of people. #Person2# thinks that there are many people who follow rugby in his country.\n    \n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n     #Person1# and #Person2# talk about #Person2#'s diet. #Person1# offers to get #Person2# a salad. #Person2# accepts. #Person1# then offers to get #Person2# some soup from the restaurant. #Person2# accepts. #Person1# then invites #Person2# to the party afterwards.\n    \n\n     #Person1# invites #Person2# to come into his office. #Person2# greets #Person1# and says, \"Good morning. Thank you.\" #Person1# asks #Person2# if he has any writing experience and #Person2# says, \"Yes, I have written for several top newspapers in the country. I'm also in the process of writing my first novel.\" #Person1# tells #Person2# why he wants him on staff.\n    \n","output_type":"stream"}]},{"cell_type":"code","source":"output_df = pd.DataFrame(wandb_summary,columns=['Prompt', 'Ground Truth', 'Model Answer','BLEU_SCORE','Precision', 'Recall','f1'])\navg_BLEU = output_df['BLEU_SCORE'].mean()\navg_precision = output_df['Precision'].mean()\navg_recall = output_df['Recall'].mean()\navg_f1 = output_df['f1'].mean()\noutput_df","metadata":{"execution":{"iopub.status.busy":"2024-11-12T08:03:03.188172Z","iopub.execute_input":"2024-11-12T08:03:03.188594Z","iopub.status.idle":"2024-11-12T08:03:03.215648Z","shell.execute_reply.started":"2024-11-12T08:03:03.188557Z","shell.execute_reply":"2024-11-12T08:03:03.214793Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                               Prompt  \\\n0   \\n    Summarize the following conversation.\\n\\...   \n1   \\n    Summarize the following conversation.\\n\\...   \n2   \\n    Summarize the following conversation.\\n\\...   \n3   \\n    Summarize the following conversation.\\n\\...   \n4   \\n    Summarize the following conversation.\\n\\...   \n5   \\n    Summarize the following conversation.\\n\\...   \n6   \\n    Summarize the following conversation.\\n\\...   \n7   \\n    Summarize the following conversation.\\n\\...   \n8   \\n    Summarize the following conversation.\\n\\...   \n9   \\n    Summarize the following conversation.\\n\\...   \n10  \\n    Summarize the following conversation.\\n\\...   \n11  \\n    Summarize the following conversation.\\n\\...   \n12  \\n    Summarize the following conversation.\\n\\...   \n13  \\n    Summarize the following conversation.\\n\\...   \n14  \\n    Summarize the following conversation.\\n\\...   \n15  \\n    Summarize the following conversation.\\n\\...   \n16  \\n    Summarize the following conversation.\\n\\...   \n17  \\n    Summarize the following conversation.\\n\\...   \n18  \\n    Summarize the following conversation.\\n\\...   \n19  \\n    Summarize the following conversation.\\n\\...   \n\n                                         Ground Truth  \\\n0   #Person1# and #Person2# talk about pilgrims ar...   \n1   #Person1# is sending a package with the help o...   \n2   #Person1# invites Lily to picnic this weekend....   \n3      #Person1# makes drinks for #Person2# at a bar.   \n4   Lin Fang and Lucy are talking about their favo...   \n5   #Person2# tells #Person1# that he falls in lov...   \n6   Mr. Smith asks #Person1# to help find his trai...   \n7   #Person1# congratulates #Person2# on #Person2#...   \n8   #Person1# gets a crank call and is angry about...   \n9   #Person1# introduces the weekend driving cours...   \n10  #Person1# and #Person2# are talking about the ...   \n11  #Person1# and #Person2# are talking about the ...   \n12  Mark asks Maggie for her notes because Mark's ...   \n13  Joe doesn't think highly of Tom's new assistan...   \n14  #Person1# and #Person2# are talking about the ...   \n15  #Person1# thinks #Person2# has chicken pox and...   \n16  #Person1# gives #Person2# a physical check and...   \n17  People in #Person2#'s country like football, b...   \n18  Sue doesn't eat cakes and sandwiches at her bi...   \n19  #Person2# tells #Person1# #Person2#'s writing ...   \n\n                                         Model Answer  BLEU_SCORE  Precision  \\\n0   \\n     #Person1# and #Person2# are talking abo...    0.117949   0.869010   \n1   \\n     #Person1# wants to send a package by fi...    0.075221   0.861336   \n2   \\n     Lily will take part in the picnic at th...    0.035912   0.782955   \n3   \\n     #Person1# can get #Person2# something t...    0.048951   0.845604   \n4   \\n     Lin Fang and Lucy have different favori...    0.018519   0.742482   \n5   \\n     #Person1# is talking about a neighbor h...    0.112903   0.894168   \n6   \\n     Person 1: How can I help you?\\n     Per...    0.024922   0.766385   \n7   \\n     Person 1. You're going to set up your o...    0.016746   0.800838   \n8   \\n     #Person1# is calling #Person2# who he k...    0.047619   0.842209   \n9   \\n     Person 1: Good afternoon, what can I do...    0.011662   0.769268   \n10  \\n     Person1: Are you familiar with American...    0.046479   0.776226   \n11  \\n     #Person1# thinks spring is finally here...    0.105263   0.884479   \n12  \\n     Mark needs someone to study with and Ma...    0.029870   0.791592   \n13  \\n     Tom and Emm are talking about Emm's new...    0.008043   0.773347   \n14  \\n     #Person1# likes football, basketball, a...    0.108280   0.870556   \n15  \\n     #Person1# and #Person2# are talking abo...    0.070033   0.792269   \n16  \\n     Person 1 asks Person 2 to take off his ...    0.026984   0.828621   \n17  \\n     #Person1# likes football, basketball, s...    0.183486   0.900174   \n18  \\n     #Person1# and #Person2# talk about #Per...    0.054264   0.812502   \n19  \\n     #Person1# invites #Person2# to come int...    0.073446   0.853847   \n\n      Recall        f1  \n0   0.894844  0.881738  \n1   0.913869  0.886825  \n2   0.875399  0.826600  \n3   0.909002  0.876157  \n4   0.881741  0.806142  \n5   0.890143  0.892151  \n6   0.850543  0.806274  \n7   0.805873  0.803347  \n8   0.892153  0.866462  \n9   0.839741  0.802961  \n10  0.873302  0.821908  \n11  0.921796  0.902752  \n12  0.859620  0.824205  \n13  0.876562  0.821726  \n14  0.888667  0.879518  \n15  0.895615  0.840778  \n16  0.871850  0.849686  \n17  0.904455  0.902310  \n18  0.876503  0.843289  \n19  0.885584  0.869426  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Prompt</th>\n      <th>Ground Truth</th>\n      <th>Model Answer</th>\n      <th>BLEU_SCORE</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>#Person1# and #Person2# talk about pilgrims ar...</td>\n      <td>\\n     #Person1# and #Person2# are talking abo...</td>\n      <td>0.117949</td>\n      <td>0.869010</td>\n      <td>0.894844</td>\n      <td>0.881738</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>#Person1# is sending a package with the help o...</td>\n      <td>\\n     #Person1# wants to send a package by fi...</td>\n      <td>0.075221</td>\n      <td>0.861336</td>\n      <td>0.913869</td>\n      <td>0.886825</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>#Person1# invites Lily to picnic this weekend....</td>\n      <td>\\n     Lily will take part in the picnic at th...</td>\n      <td>0.035912</td>\n      <td>0.782955</td>\n      <td>0.875399</td>\n      <td>0.826600</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>#Person1# makes drinks for #Person2# at a bar.</td>\n      <td>\\n     #Person1# can get #Person2# something t...</td>\n      <td>0.048951</td>\n      <td>0.845604</td>\n      <td>0.909002</td>\n      <td>0.876157</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>Lin Fang and Lucy are talking about their favo...</td>\n      <td>\\n     Lin Fang and Lucy have different favori...</td>\n      <td>0.018519</td>\n      <td>0.742482</td>\n      <td>0.881741</td>\n      <td>0.806142</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>#Person2# tells #Person1# that he falls in lov...</td>\n      <td>\\n     #Person1# is talking about a neighbor h...</td>\n      <td>0.112903</td>\n      <td>0.894168</td>\n      <td>0.890143</td>\n      <td>0.892151</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>Mr. Smith asks #Person1# to help find his trai...</td>\n      <td>\\n     Person 1: How can I help you?\\n     Per...</td>\n      <td>0.024922</td>\n      <td>0.766385</td>\n      <td>0.850543</td>\n      <td>0.806274</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>#Person1# congratulates #Person2# on #Person2#...</td>\n      <td>\\n     Person 1. You're going to set up your o...</td>\n      <td>0.016746</td>\n      <td>0.800838</td>\n      <td>0.805873</td>\n      <td>0.803347</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>#Person1# gets a crank call and is angry about...</td>\n      <td>\\n     #Person1# is calling #Person2# who he k...</td>\n      <td>0.047619</td>\n      <td>0.842209</td>\n      <td>0.892153</td>\n      <td>0.866462</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>#Person1# introduces the weekend driving cours...</td>\n      <td>\\n     Person 1: Good afternoon, what can I do...</td>\n      <td>0.011662</td>\n      <td>0.769268</td>\n      <td>0.839741</td>\n      <td>0.802961</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>#Person1# and #Person2# are talking about the ...</td>\n      <td>\\n     Person1: Are you familiar with American...</td>\n      <td>0.046479</td>\n      <td>0.776226</td>\n      <td>0.873302</td>\n      <td>0.821908</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>#Person1# and #Person2# are talking about the ...</td>\n      <td>\\n     #Person1# thinks spring is finally here...</td>\n      <td>0.105263</td>\n      <td>0.884479</td>\n      <td>0.921796</td>\n      <td>0.902752</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>Mark asks Maggie for her notes because Mark's ...</td>\n      <td>\\n     Mark needs someone to study with and Ma...</td>\n      <td>0.029870</td>\n      <td>0.791592</td>\n      <td>0.859620</td>\n      <td>0.824205</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>Joe doesn't think highly of Tom's new assistan...</td>\n      <td>\\n     Tom and Emm are talking about Emm's new...</td>\n      <td>0.008043</td>\n      <td>0.773347</td>\n      <td>0.876562</td>\n      <td>0.821726</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>#Person1# and #Person2# are talking about the ...</td>\n      <td>\\n     #Person1# likes football, basketball, a...</td>\n      <td>0.108280</td>\n      <td>0.870556</td>\n      <td>0.888667</td>\n      <td>0.879518</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>#Person1# thinks #Person2# has chicken pox and...</td>\n      <td>\\n     #Person1# and #Person2# are talking abo...</td>\n      <td>0.070033</td>\n      <td>0.792269</td>\n      <td>0.895615</td>\n      <td>0.840778</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>#Person1# gives #Person2# a physical check and...</td>\n      <td>\\n     Person 1 asks Person 2 to take off his ...</td>\n      <td>0.026984</td>\n      <td>0.828621</td>\n      <td>0.871850</td>\n      <td>0.849686</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>People in #Person2#'s country like football, b...</td>\n      <td>\\n     #Person1# likes football, basketball, s...</td>\n      <td>0.183486</td>\n      <td>0.900174</td>\n      <td>0.904455</td>\n      <td>0.902310</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>Sue doesn't eat cakes and sandwiches at her bi...</td>\n      <td>\\n     #Person1# and #Person2# talk about #Per...</td>\n      <td>0.054264</td>\n      <td>0.812502</td>\n      <td>0.876503</td>\n      <td>0.843289</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>\\n    Summarize the following conversation.\\n\\...</td>\n      <td>#Person2# tells #Person1# #Person2#'s writing ...</td>\n      <td>\\n     #Person1# invites #Person2# to come int...</td>\n      <td>0.073446</td>\n      <td>0.853847</td>\n      <td>0.885584</td>\n      <td>0.869426</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"col2str = {x: 'string' for x in ['Prompt', 'Ground Truth', 'Model Answer']}\nwandb.log({'avg_BLEU':avg_BLEU})\nwandb.log({'avg_precision':avg_precision})\nwandb.log({'avg_recall':avg_recall})\nwandb.log({'avg_f1':avg_f1})\nwandb.log({'results': wandb.Table(dataframe=output_df.astype(col2str))})","metadata":{"execution":{"iopub.status.busy":"2024-11-12T08:03:15.397974Z","iopub.execute_input":"2024-11-12T08:03:15.398472Z","iopub.status.idle":"2024-11-12T08:03:15.807019Z","shell.execute_reply.started":"2024-11-12T08:03:15.398427Z","shell.execute_reply":"2024-11-12T08:03:15.806077Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Load and merge qlora weights using this [script](https://gist.github.com/ChrisHayduk/1a53463331f52dca205e55982baf9930)","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-11-07T03:12:37.743220Z","iopub.execute_input":"2024-11-07T03:12:37.743889Z","iopub.status.idle":"2024-11-07T03:12:38.040868Z","shell.execute_reply.started":"2024-11-07T03:12:37.743854Z","shell.execute_reply":"2024-11-07T03:12:38.039984Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e9b39eecde945a69bb0ee629b5b654b"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install peft\n!pip install bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-11-07T03:12:44.409952Z","iopub.execute_input":"2024-11-07T03:12:44.410322Z","iopub.status.idle":"2024-11-07T03:13:13.034978Z","shell.execute_reply.started":"2024-11-07T03:12:44.410286Z","shell.execute_reply":"2024-11-07T03:13:13.033866Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\nThe code below combines approaches published by both @eugene-yh and @jinyongyoo on Github. \nThanks for the contributions guys!\n\"\"\"\n\nimport torch\nimport peft\nimport json\nimport shutil\nfrom peft.utils import _get_submodules\nimport os\nimport bitsandbytes as bnb\nfrom bitsandbytes.functional import dequantize_4bit\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig, CodeLlamaTokenizer,AutoTokenizer\nimport gc\nimport copy\n\ndef save_model(model, tokenizer, to):\n    print(f\"Saving dequantized model to {to}...\")\n    model.save_pretrained(to)\n    tokenizer.save_pretrained(to)\n    config_data = json.loads(open(os.path.join(to, 'config.json'), 'r').read())\n    config_data.pop(\"quantization_config\", None)  # Remove quantization config\n    config_data.pop(\"pretraining_tp\", None)  # Remove any other TP settings\n    with open(os.path.join(to, 'config.json'), 'w') as config:\n        config.write(json.dumps(config_data, indent=2))\n    \ndef dequantize_model(model, tokenizer, to='./dequantized_model', dtype=torch.bfloat16, device=\"cpu\"):\n    \"\"\"\n    'model': the peftmodel you loaded with qlora.\n    'tokenizer': the model's corresponding hf's tokenizer.\n    'to': directory to save the dequantized model\n    'dtype': dtype that the model was trained using\n    'device': device to load the model to\n    \"\"\"\n\n    # Delete the model object if it exists\n    if os.path.exists(to):\n        shutil.rmtree(to)\n\n    os.makedirs(to, exist_ok=True)\n\n    cls = bnb.nn.Linear4bit\n\n    with torch.no_grad():\n        for name, module in model.named_modules():\n            if isinstance(module, cls):\n                print(f\"Dequantizing `{name}`...\")\n                quant_state = copy.deepcopy(module.weight.quant_state)\n\n                quant_state.dtype = dtype\n\n                weights = dequantize_4bit(module.weight.data, quant_state=quant_state, quant_type=\"nf4\").to(dtype)\n\n                new_module = torch.nn.Linear(module.in_features, module.out_features, bias=None, dtype=dtype)\n                new_module.weight = torch.nn.Parameter(weights)\n                new_module.to(device=device, dtype=dtype)\n\n                parent, target, target_name = _get_submodules(model, name)\n                setattr(parent, target_name, new_module)\n\n        model.is_loaded_in_4bit = False\n\n        save_model(model, tokenizer, to)\n        \n        return model\n        \n\nmodel_path = \"meta-llama/Llama-3.2-1B\"\nadapter_path = \"WeiWen21/ww-fine-tuned-adapter-v9\"\n\nquantization_config=BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n\ntry:\n    print(f\"Starting to load the model {model_path} into memory\")\n\n    model = LlamaForCausalLM.from_pretrained(\n        model_path,\n        torch_dtype=torch.bfloat16,\n        quantization_config=quantization_config,\n        device_map=\"cuda\"\n    )\n    print(model)\n    \n    tok = AutoTokenizer.from_pretrained(model_path)\n    \n    model.resize_token_embeddings(len(tok))\n    \n    # Note: This function outputs the dequantized model without merging the adapter yet\n    # The code below it will merge the adapter and then save it to disk\n    model = dequantize_model(model, tok, to='dequantized-model')\n    model.eval()\n    print(model)\n    model = PeftModel.from_pretrained(model = model, model_id = adapter_path, ignore_mismatched_sizes=True)\n    print(model)\n    model = model.merge_and_unload()\n    print(model)\n    \n    print(f\"Successfully loaded the model {model_path} into memory\")\n    \n    # Note that the output folder here should be different than the one you used for dequantize_model\n    # This save will output the model merged with LoRA weights\n    save_model(model, tok, \"ww_merged\")\n    \n    print(f\"Successfully saved merged model {model_path} to disk\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\n    # Delete the model object if it exists\n    if 'model' in locals():\n        del model\n\n    # Clear the GPU cache\n    torch.cuda.empty_cache()\n\n    # Run the garbage collection\n    gc.collect()\n\n    print(\"Model, GPU cache, and garbage have been cleared.\")","metadata":{"execution":{"iopub.status.busy":"2024-11-07T03:15:59.755507Z","iopub.execute_input":"2024-11-07T03:15:59.755870Z","iopub.status.idle":"2024-11-07T03:16:41.033246Z","shell.execute_reply.started":"2024-11-07T03:15:59.755837Z","shell.execute_reply":"2024-11-07T03:16:41.032175Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Starting to load the model meta-llama/Llama-3.2-1B into memory\nLlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a78ca959c1b4096863e1028a9b6895a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28954001cf18488bbfa27eef4386374b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bff7f2971fd14198ad8df74077e34faf"}},"metadata":{}},{"name":"stdout","text":"Dequantizing `model.layers.0.self_attn.q_proj`...\nDequantizing `model.layers.0.self_attn.k_proj`...\nDequantizing `model.layers.0.self_attn.v_proj`...\nDequantizing `model.layers.0.self_attn.o_proj`...\nDequantizing `model.layers.0.mlp.gate_proj`...\nDequantizing `model.layers.0.mlp.up_proj`...\nDequantizing `model.layers.0.mlp.down_proj`...\nDequantizing `model.layers.1.self_attn.q_proj`...\nDequantizing `model.layers.1.self_attn.k_proj`...\nDequantizing `model.layers.1.self_attn.v_proj`...\nDequantizing `model.layers.1.self_attn.o_proj`...\nDequantizing `model.layers.1.mlp.gate_proj`...\nDequantizing `model.layers.1.mlp.up_proj`...\nDequantizing `model.layers.1.mlp.down_proj`...\nDequantizing `model.layers.2.self_attn.q_proj`...\nDequantizing `model.layers.2.self_attn.k_proj`...\nDequantizing `model.layers.2.self_attn.v_proj`...\nDequantizing `model.layers.2.self_attn.o_proj`...\nDequantizing `model.layers.2.mlp.gate_proj`...\nDequantizing `model.layers.2.mlp.up_proj`...\nDequantizing `model.layers.2.mlp.down_proj`...\nDequantizing `model.layers.3.self_attn.q_proj`...\nDequantizing `model.layers.3.self_attn.k_proj`...\nDequantizing `model.layers.3.self_attn.v_proj`...\nDequantizing `model.layers.3.self_attn.o_proj`...\nDequantizing `model.layers.3.mlp.gate_proj`...\nDequantizing `model.layers.3.mlp.up_proj`...\nDequantizing `model.layers.3.mlp.down_proj`...\nDequantizing `model.layers.4.self_attn.q_proj`...\nDequantizing `model.layers.4.self_attn.k_proj`...\nDequantizing `model.layers.4.self_attn.v_proj`...\nDequantizing `model.layers.4.self_attn.o_proj`...\nDequantizing `model.layers.4.mlp.gate_proj`...\nDequantizing `model.layers.4.mlp.up_proj`...\nDequantizing `model.layers.4.mlp.down_proj`...\nDequantizing `model.layers.5.self_attn.q_proj`...\nDequantizing `model.layers.5.self_attn.k_proj`...\nDequantizing `model.layers.5.self_attn.v_proj`...\nDequantizing `model.layers.5.self_attn.o_proj`...\nDequantizing `model.layers.5.mlp.gate_proj`...\nDequantizing `model.layers.5.mlp.up_proj`...\nDequantizing `model.layers.5.mlp.down_proj`...\nDequantizing `model.layers.6.self_attn.q_proj`...\nDequantizing `model.layers.6.self_attn.k_proj`...\nDequantizing `model.layers.6.self_attn.v_proj`...\nDequantizing `model.layers.6.self_attn.o_proj`...\nDequantizing `model.layers.6.mlp.gate_proj`...\nDequantizing `model.layers.6.mlp.up_proj`...\nDequantizing `model.layers.6.mlp.down_proj`...\nDequantizing `model.layers.7.self_attn.q_proj`...\nDequantizing `model.layers.7.self_attn.k_proj`...\nDequantizing `model.layers.7.self_attn.v_proj`...\nDequantizing `model.layers.7.self_attn.o_proj`...\nDequantizing `model.layers.7.mlp.gate_proj`...\nDequantizing `model.layers.7.mlp.up_proj`...\nDequantizing `model.layers.7.mlp.down_proj`...\nDequantizing `model.layers.8.self_attn.q_proj`...\nDequantizing `model.layers.8.self_attn.k_proj`...\nDequantizing `model.layers.8.self_attn.v_proj`...\nDequantizing `model.layers.8.self_attn.o_proj`...\nDequantizing `model.layers.8.mlp.gate_proj`...\nDequantizing `model.layers.8.mlp.up_proj`...\nDequantizing `model.layers.8.mlp.down_proj`...\nDequantizing `model.layers.9.self_attn.q_proj`...\nDequantizing `model.layers.9.self_attn.k_proj`...\nDequantizing `model.layers.9.self_attn.v_proj`...\nDequantizing `model.layers.9.self_attn.o_proj`...\nDequantizing `model.layers.9.mlp.gate_proj`...\nDequantizing `model.layers.9.mlp.up_proj`...\nDequantizing `model.layers.9.mlp.down_proj`...\nDequantizing `model.layers.10.self_attn.q_proj`...\nDequantizing `model.layers.10.self_attn.k_proj`...\nDequantizing `model.layers.10.self_attn.v_proj`...\nDequantizing `model.layers.10.self_attn.o_proj`...\nDequantizing `model.layers.10.mlp.gate_proj`...\nDequantizing `model.layers.10.mlp.up_proj`...\nDequantizing `model.layers.10.mlp.down_proj`...\nDequantizing `model.layers.11.self_attn.q_proj`...\nDequantizing `model.layers.11.self_attn.k_proj`...\nDequantizing `model.layers.11.self_attn.v_proj`...\nDequantizing `model.layers.11.self_attn.o_proj`...\nDequantizing `model.layers.11.mlp.gate_proj`...\nDequantizing `model.layers.11.mlp.up_proj`...\nDequantizing `model.layers.11.mlp.down_proj`...\nDequantizing `model.layers.12.self_attn.q_proj`...\nDequantizing `model.layers.12.self_attn.k_proj`...\nDequantizing `model.layers.12.self_attn.v_proj`...\nDequantizing `model.layers.12.self_attn.o_proj`...\nDequantizing `model.layers.12.mlp.gate_proj`...\nDequantizing `model.layers.12.mlp.up_proj`...\nDequantizing `model.layers.12.mlp.down_proj`...\nDequantizing `model.layers.13.self_attn.q_proj`...\nDequantizing `model.layers.13.self_attn.k_proj`...\nDequantizing `model.layers.13.self_attn.v_proj`...\nDequantizing `model.layers.13.self_attn.o_proj`...\nDequantizing `model.layers.13.mlp.gate_proj`...\nDequantizing `model.layers.13.mlp.up_proj`...\nDequantizing `model.layers.13.mlp.down_proj`...\nDequantizing `model.layers.14.self_attn.q_proj`...\nDequantizing `model.layers.14.self_attn.k_proj`...\nDequantizing `model.layers.14.self_attn.v_proj`...\nDequantizing `model.layers.14.self_attn.o_proj`...\nDequantizing `model.layers.14.mlp.gate_proj`...\nDequantizing `model.layers.14.mlp.up_proj`...\nDequantizing `model.layers.14.mlp.down_proj`...\nDequantizing `model.layers.15.self_attn.q_proj`...\nDequantizing `model.layers.15.self_attn.k_proj`...\nDequantizing `model.layers.15.self_attn.v_proj`...\nDequantizing `model.layers.15.self_attn.o_proj`...\nDequantizing `model.layers.15.mlp.gate_proj`...\nDequantizing `model.layers.15.mlp.up_proj`...\nDequantizing `model.layers.15.mlp.down_proj`...\nSaving dequantized model to dequantized-model...\nLlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d23f10c403f40cebc94278bbb81c74f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/22.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"999c9598f5c24164a5a1d27e3074f9d1"}},"metadata":{}},{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128256, 2048)\n        (layers): ModuleList(\n          (0-15): 16 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=8192, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n    )\n  )\n)\nLlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)\nSuccessfully loaded the model meta-llama/Llama-3.2-1B into memory\nSaving dequantized model to ww_merged...\nSuccessfully saved merged model meta-llama/Llama-3.2-1B to disk\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Push the merged model","metadata":{}},{"cell_type":"code","source":"tok.push_to_hub(\"WeiWen21/fine-tuned-merged-model-v6\")\nmodel.push_to_hub(\"WeiWen21/fine-tuned-merged-model-v6\")","metadata":{"execution":{"iopub.status.busy":"2024-11-07T03:17:47.877311Z","iopub.execute_input":"2024-11-07T03:17:47.877868Z","iopub.status.idle":"2024-11-07T03:19:23.328160Z","shell.execute_reply.started":"2024-11-07T03:17:47.877829Z","shell.execute_reply":"2024-11-07T03:19:23.327233Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"793cf46225a94726b2f6c36d9e58f039"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f56b4eb64a44c2a8439c69420386a18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35b98a79e7d14628b622525953517be8"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/WeiWen21/fine-tuned-merged-model-v6/commit/c82b3c7cda3ee17ba45a60a3213fc70e4e099823', commit_message='Upload LlamaForCausalLM', commit_description='', oid='c82b3c7cda3ee17ba45a60a3213fc70e4e099823', pr_url=None, repo_url=RepoUrl('https://huggingface.co/WeiWen21/fine-tuned-merged-model-v6', endpoint='https://huggingface.co', repo_type='model', repo_id='WeiWen21/fine-tuned-merged-model-v6'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"model.to('cuda')\nprompt = \"\"\"\nSummarize the following conversation.\n\nConversation:\n#Person1#: Mister Ewing said we should show up at the conference center at 4:00 o'clock, right?\n#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are coming, and he wants to make a good impression on them. How are you getting there?\n#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is construction on the highway. What about you?\n#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center only once, and I'm not sure if I can find my way around there.\n\nSummary:\n\"\"\"\ninputs = tok(prompt, return_tensors='pt').to('cuda')\noutput = tok.decode(\n    model.generate(\n        inputs[\"input_ids\"], \n        max_new_tokens=200,\n    )[0], \n    skip_special_tokens=True\n)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(output)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-07T03:19:26.431319Z","iopub.execute_input":"2024-11-07T03:19:26.431762Z","iopub.status.idle":"2024-11-07T03:19:40.756670Z","shell.execute_reply.started":"2024-11-07T03:19:26.431724Z","shell.execute_reply":"2024-11-07T03:19:40.755673Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"INPUT PROMPT:\n\nSummarize the following conversation.\n\nConversation:\n#Person1#: Mister Ewing said we should show up at the conference center at 4:00 o'clock, right?\n#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are coming, and he wants to make a good impression on them. How are you getting there?\n#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is construction on the highway. What about you?\n#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center only once, and I'm not sure if I can find my way around there.\n\nSummary:\n\n\nSummarize the following conversation.\n\nConversation:\n#Person1#: Mister Ewing said we should show up at the conference center at 4:00 o'clock, right?\n#Person2#: Yes, he especially asked us not to be late. Some of the people from our east york branch office are coming, and he wants to make a good impression on them. How are you getting there?\n#Person1#: I was thinking of taking my car, but I think I'm just going to take the underground, because there is construction on the highway. What about you?\n#Person2#: I'll be taking the underground as well. Why don't we go together? I've been to the conference center only once, and I'm not sure if I can find my way around there.\n\nSummary:\n#Person1# and #Person2# plan to take the underground together to the conference center because Mr. Ewing asks them not to be late.\n\n","output_type":"stream"}]}]}